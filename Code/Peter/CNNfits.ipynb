{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a CNN with a .fits file as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras import utils as np_utils\n",
    "import tensorflow\n",
    "from keras import datasets, layers, models\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import joblib\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imread\n",
    "from init_modules import *\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global Variables'\n",
    "data_set = {}\n",
    "im_size = 100 + 2  # + 2 is needed to acocunt for 0 indexing in arrays\n",
    "\n",
    "shift_interval = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function finds the position of the object in the image\n",
    "def find_object_pos(file):\n",
    "    print(file)\n",
    "    cd = ClustarData(path=file, group_factor=0)\n",
    "    if len(cd.groups) > 0:\n",
    "        disk = cd.groups[0]\n",
    "        bounds = disk.image.bounds\n",
    "        x = (bounds[2] + bounds[3])/2\n",
    "        y = (bounds[0] + bounds[1])/2\n",
    "        return (x, y)\n",
    "    else:\n",
    "        # img_data = fits.getdata(file)\n",
    "        \n",
    "        # return img_data.data.shape[0]/2, img_data.data.shape[1]/2\n",
    "        print(\"No object found in {}\".format(file))\n",
    "        return None\n",
    "\n",
    "\n",
    "# This function crops the image to the size of the object\n",
    "def init_cropped_images(directory_of_fits_files):\n",
    "    fits_files = []\n",
    "    for fits_file in directory_of_fits_files:\n",
    "        img_data = fits.getdata(fits_file)\n",
    "\n",
    "        if (img_data.shape) == 2:\n",
    "            img_data.shape = [1, 1, img_data.shape[0], img_data.shape[1]]\n",
    "\n",
    "        object_pos = find_object_pos(fits_file)\n",
    "\n",
    "        if object_pos != None:\n",
    "            # Data shape is (1, 1, x, y) we want it to be (x, y)\n",
    "            if len(img_data.shape) > 2:\n",
    "                img_data.shape = (img_data.shape[2], img_data.shape[3])\n",
    "\n",
    "            # Set the size of the crop in pixels | Since we will rotate the image, we need to add some padding to then be able to shift the image and still have the object in the image\n",
    "            crop_size = units.Quantity((im_size + 50, im_size + 50), units.pixel)\n",
    "\n",
    "            img_crop = Cutout2D(img_data, object_pos, crop_size)\n",
    "\n",
    "            fits_files.append(img_crop)\n",
    "\n",
    "    return fits_files\n",
    "\n",
    "\n",
    "# This function rotates the image by a random angle and shifts it by a random amount in a random direction\n",
    "def rotate_disk(disk_to_rotate, angle):\n",
    "\n",
    "    # Rotate the disk\n",
    "    rotated_disk = rotate(disk_to_rotate, angle)\n",
    "\n",
    "    # Since rotating pads the image, we need to crop it to the original size\n",
    "    x, y = (len(rotated_disk[0]), len(rotated_disk))\n",
    "\n",
    "    shift_interval = 0\n",
    "    si = shift_interval + 1\n",
    "\n",
    "    rand_x_shift = random.randint(-shift_interval, shift_interval)\n",
    "    rand_y_shift = random.randint(-shift_interval, shift_interval)\n",
    "\n",
    "    (x_lower, x_upper) = int((x/2 - im_size/2)) + \\\n",
    "        rand_x_shift, int(x/2 + im_size/2) + rand_x_shift\n",
    "    \n",
    "    (y_lower, y_upper) = int((y/2 - im_size/2)) + \\\n",
    "        rand_y_shift, int(y/2 + im_size/2) + rand_y_shift\n",
    "\n",
    "    return rotated_disk[(x_lower+si):(x_upper-si), (y_lower+si):(y_upper-si)]\n",
    "\n",
    "\n",
    "# This function flips the image horizontally, vertically or both\n",
    "def flip_disk(disk_to_flip):\n",
    "\n",
    "    flipped_disk = disk_to_flip\n",
    "\n",
    "    if bool(random.getrandbits(1)):\n",
    "        flipped_disk = np.fliplr(flipped_disk)\n",
    "\n",
    "    if bool(random.getrandbits(1)):\n",
    "        flipped_disk = np.flipud(flipped_disk)\n",
    "\n",
    "    if bool(random.getrandbits(1)):\n",
    "        flipped_disk = np.flip(flipped_disk)\n",
    "\n",
    "    return flipped_disk\n",
    "\n",
    "\n",
    "# This function augments the image by rotating and flipping it\n",
    "def augment_disk(disk):\n",
    "    angle = random.randint(0, 360)\n",
    "    return rotate_disk(flip_disk(disk), angle)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate dataset from the fits files\n",
    "\n",
    "def generate_dataset(augmentations_per_gaussian, directory_of_fits_files):\n",
    "\n",
    "    dataset = []\n",
    "    fits_files = init_cropped_images(directory_of_fits_files)\n",
    "    \n",
    "    for fits_file in fits_files:\n",
    "        for i in range(0, augmentations_per_gaussian):\n",
    "            #Since we add +50 in the init_cropped_images function, we need to check if the image is the correct size\n",
    "            if fits_file.data.shape == (im_size + 50, im_size + 50):\n",
    "                dataset.append(augment_disk(fits_file.data))         \n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/train_pos\\b335_2017_band6_0.fits\n",
      "data/train_pos\\b335_2018_band7_0.fits\n",
      "data/train_pos\\hh212_2015_band7_0.fits\n",
      "No object found in data/train_pos\\hh212_2015_band7_0.fits\n",
      "data/train_pos\\hh212_2015_band7_1.fits\n",
      "No object found in data/train_pos\\hh212_2015_band7_1.fits\n",
      "data/train_pos\\hh212_2016_band7_0.fits\n",
      "data/train_pos\\hh212_2016_band7_1.fits\n",
      "data/train_pos\\pos01.fits\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 2-dimensional, but 4 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(generate_dataset(\u001b[39m2\u001b[39;49m, glob\u001b[39m.\u001b[39;49mglob(\u001b[39m'\u001b[39;49m\u001b[39mdata/train_pos/*.fits\u001b[39;49m\u001b[39m'\u001b[39;49m)))\n",
      "Cell \u001b[1;32mIn[11], line 6\u001b[0m, in \u001b[0;36mgenerate_dataset\u001b[1;34m(augmentations_per_gaussian, directory_of_fits_files)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_dataset\u001b[39m(augmentations_per_gaussian, directory_of_fits_files):\n\u001b[0;32m      5\u001b[0m     dataset \u001b[39m=\u001b[39m []\n\u001b[1;32m----> 6\u001b[0m     fits_files \u001b[39m=\u001b[39m init_cropped_images(directory_of_fits_files)\n\u001b[0;32m      8\u001b[0m     \u001b[39mfor\u001b[39;00m fits_file \u001b[39min\u001b[39;00m fits_files:\n\u001b[0;32m      9\u001b[0m         \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, augmentations_per_gaussian):\n\u001b[0;32m     10\u001b[0m             \u001b[39m#Since we add +50 in the init_cropped_images function, we need to check if the image is the correct size\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[27], line 28\u001b[0m, in \u001b[0;36minit_cropped_images\u001b[1;34m(directory_of_fits_files)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39mif\u001b[39;00m (img_data\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m     26\u001b[0m     img_data\u001b[39m.\u001b[39mshape \u001b[39m=\u001b[39m [\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, img_data\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], img_data\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]]\n\u001b[1;32m---> 28\u001b[0m object_pos \u001b[39m=\u001b[39m find_object_pos(fits_file)\n\u001b[0;32m     30\u001b[0m \u001b[39mif\u001b[39;00m object_pos \u001b[39m!=\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     31\u001b[0m     \u001b[39m# Data shape is (1, 1, x, y) we want it to be (x, y)\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(img_data\u001b[39m.\u001b[39mshape) \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m:\n",
      "Cell \u001b[1;32mIn[27], line 4\u001b[0m, in \u001b[0;36mfind_object_pos\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfind_object_pos\u001b[39m(file):\n\u001b[0;32m      3\u001b[0m     \u001b[39mprint\u001b[39m(file)\n\u001b[1;32m----> 4\u001b[0m     cd \u001b[39m=\u001b[39m ClustarData(path\u001b[39m=\u001b[39;49mfile, group_factor\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m      5\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(cd\u001b[39m.\u001b[39mgroups) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m      6\u001b[0m         disk \u001b[39m=\u001b[39m cd\u001b[39m.\u001b[39mgroups[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\ChalmersWorkspaces\\KandidatArbete\\almaEnv\\lib\\site-packages\\clustar\\core.py:549\u001b[0m, in \u001b[0;36mClustarData.__init__\u001b[1;34m(self, path, **kwargs)\u001b[0m\n\u001b[0;32m    547\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups \u001b[39m=\u001b[39m []\n\u001b[0;32m    548\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflag \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m--> 549\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load_file()\n\u001b[0;32m    550\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setup()\n",
      "File \u001b[1;32mc:\\ChalmersWorkspaces\\KandidatArbete\\almaEnv\\lib\\site-packages\\clustar\\core.py:554\u001b[0m, in \u001b[0;36mClustarData._load_file\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    552\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_load_file\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    553\u001b[0m     file \u001b[39m=\u001b[39m astropy\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mfits\u001b[39m.\u001b[39mopen(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpath)\n\u001b[1;32m--> 554\u001b[0m     data \u001b[39m=\u001b[39m file[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mdata[\u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m, :, :]\n\u001b[0;32m    555\u001b[0m     header \u001b[39m=\u001b[39m file[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mheader\n\u001b[0;32m    556\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mImage(data, header)\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array: array is 2-dimensional, but 4 were indexed"
     ]
    }
   ],
   "source": [
    "print(generate_dataset(2, glob.glob('data/train_pos/*.fits')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m\n\u001b[0;32m      6\u001b[0m img_rows, img_cols \u001b[39m=\u001b[39m im_size, im_size  \u001b[39m# sqrt of 6724\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m x_train \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(generate_dataset(nmbr_of_aug, glob\u001b[39m.\u001b[39;49mglob(\u001b[39m'\u001b[39;49m\u001b[39mdata/train_pos/*.fits\u001b[39;49m\u001b[39m'\u001b[39;49m)) \u001b[39m+\u001b[39m\n\u001b[0;32m      9\u001b[0m                    generate_dataset(nmbr_of_aug, glob\u001b[39m.\u001b[39mglob(\u001b[39m'\u001b[39m\u001b[39mdata/train_neg/*.fits\u001b[39m\u001b[39m'\u001b[39m)))\n\u001b[0;32m     11\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39mlen\u001b[39m(x_train)\u001b[39m/\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(x_train))\n",
      "Cell \u001b[1;32mIn[7], line 6\u001b[0m, in \u001b[0;36mgenerate_dataset\u001b[1;34m(augmentations_per_gaussian, directory_of_fits_files)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_dataset\u001b[39m(augmentations_per_gaussian, directory_of_fits_files):\n\u001b[0;32m      5\u001b[0m     dataset \u001b[39m=\u001b[39m []\n\u001b[1;32m----> 6\u001b[0m     fits_files \u001b[39m=\u001b[39m init_cropped_images(directory_of_fits_files)\n\u001b[0;32m      8\u001b[0m     \u001b[39mfor\u001b[39;00m fits_file \u001b[39min\u001b[39;00m fits_files:\n\u001b[0;32m      9\u001b[0m         \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, augmentations_per_gaussian):\n\u001b[0;32m     10\u001b[0m             \u001b[39m#Since we add +50 in the init_cropped_images function, we need to check if the image is the correct size\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 25\u001b[0m, in \u001b[0;36minit_cropped_images\u001b[1;34m(directory_of_fits_files)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mfor\u001b[39;00m fits_file \u001b[39min\u001b[39;00m directory_of_fits_files:\n\u001b[0;32m     23\u001b[0m     img_data \u001b[39m=\u001b[39m fits\u001b[39m.\u001b[39mgetdata(fits_file)\n\u001b[1;32m---> 25\u001b[0m     object_pos \u001b[39m=\u001b[39m find_object_pos(fits_file)\n\u001b[0;32m     27\u001b[0m     \u001b[39mif\u001b[39;00m object_pos \u001b[39m!=\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     28\u001b[0m         \u001b[39m# Data shape is (1, 1, x, y) we want it to be (x, y)\u001b[39;00m\n\u001b[0;32m     29\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(img_data\u001b[39m.\u001b[39mshape) \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m:\n",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m, in \u001b[0;36mfind_object_pos\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfind_object_pos\u001b[39m(file):\n\u001b[1;32m----> 3\u001b[0m     file\u001b[39m.\u001b[39;49mdata\n\u001b[0;32m      4\u001b[0m     cd \u001b[39m=\u001b[39m ClustarData(path\u001b[39m=\u001b[39mfile, group_factor\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m      5\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(cd\u001b[39m.\u001b[39mgroups) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters data-loading and formatting\n",
    "nmbr_of_aug = 15\n",
    "batch_size = 128\n",
    "num_classes = 2\n",
    "epochs = 3\n",
    "img_rows, img_cols = im_size, im_size  # sqrt of 6724\n",
    "\n",
    "x_train = np.array(generate_dataset(nmbr_of_aug, glob.glob('data/train_pos/*.fits')) +\n",
    "                   generate_dataset(nmbr_of_aug, glob.glob('data/train_neg/*.fits')))\n",
    "\n",
    "batch_size = int(len(x_train)/10)\n",
    "\n",
    "print(len(x_train))\n",
    "print(x_train[0])\n",
    "print(x_train[0].shape)\n",
    "\n",
    "\n",
    "lbl_train = [0] * len(generate_dataset(nmbr_of_aug, glob.glob('data/train_pos/*.fits'))) + \\\n",
    "            [1] * len(generate_dataset(nmbr_of_aug, glob.glob('data/train_neg/*.fits')))\n",
    "\n",
    "\n",
    "x_test = np.array(generate_dataset(nmbr_of_aug, glob.glob('data/test_pos/*.fits')) +\n",
    "                  generate_dataset(nmbr_of_aug, glob.glob('data/test_neg/*.fits')))\n",
    "\n",
    "lbl_test =  [0] * len(generate_dataset(nmbr_of_aug, glob.glob('data/test_pos/*.fits'))) + \\\n",
    "            [1] * len(generate_dataset(nmbr_of_aug, glob.glob('data/test_neg/*.fits')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 100)\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "510\n",
      "270\n"
     ]
    }
   ],
   "source": [
    "print(len(lbl_test) == len(x_test))\n",
    "print(len(lbl_train) == len(x_train))\n",
    "\n",
    "print(len(x_train))\n",
    "print(len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[129], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m x_test:\n\u001b[0;32m      9\u001b[0m     i_arr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(i)\n\u001b[1;32m---> 10\u001b[0m     x_test_temp\u001b[39m.\u001b[39;49mappend(i_arr)\n\u001b[0;32m     13\u001b[0m \u001b[39mprint\u001b[39m(x_test_temp\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     18\u001b[0m \u001b[39m# #convert x_train and x_test to png\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \n\u001b[0;32m     20\u001b[0m \u001b[39m# zscale = ZScaleInterval(contrast=0.25, nsamples=1)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[39m#     x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[39m#     input_shape = (img_rows, img_cols, 1)\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "#print(len(x_train))\n",
    "#print(x_test.shape)\n",
    "\n",
    "#print(np.array(x_test).shape)\n",
    "\n",
    "# x_test_temp = np.empty(1)\n",
    "\n",
    "# for i in x_test:\n",
    "#     i_arr = np.array(i)\n",
    "#     x_test_temp.append(i_arr)\n",
    "\n",
    "\n",
    "# print(x_test_temp.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #convert x_train and x_test to png\n",
    "\n",
    "# zscale = ZScaleInterval(contrast=0.25, nsamples=1)\n",
    "\n",
    "# for i in range(len(x_train)):\n",
    "#     x_train[i] = zscale(x_train[i])\n",
    "\n",
    "# for i in range(len(x_test)):\n",
    "#     x_test[i] = zscale(x_test[i])\n",
    "\n",
    "\n",
    "\n",
    "# if K.image_data_format() == 'channels_first':\n",
    "#     x_train = x_test.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "#     x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "#     input_shape = (1, img_rows, img_cols)\n",
    "# else:\n",
    "#     x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "#     x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "#     input_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "(98, 98)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 46 into shape (46,82,82,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[97], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m     input_shape \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m, img_rows, img_cols)\n\u001b[0;32m      8\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m----> 9\u001b[0m     x_train \u001b[39m=\u001b[39m x_train\u001b[39m.\u001b[39;49mreshape(x_train\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m], img_rows, img_cols, \u001b[39m1\u001b[39;49m)\n\u001b[0;32m     10\u001b[0m     x_test \u001b[39m=\u001b[39m x_test\u001b[39m.\u001b[39mreshape(x_test\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], img_rows, img_cols, \u001b[39m1\u001b[39m)\n\u001b[0;32m     11\u001b[0m     input_shape \u001b[39m=\u001b[39m (img_rows, img_cols, \u001b[39m1\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 46 into shape (46,82,82,1)"
     ]
    }
   ],
   "source": [
    "# print(len(x_test))\n",
    "# print(x_test[0].shape)\n",
    "\n",
    "# if K.image_data_format() == 'channels_first':\n",
    "#     x_train = x_test.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "#     x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "#     input_shape = (1, img_rows, img_cols)\n",
    "# else:\n",
    "#     x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "#     x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "#     input_shape = (img_rows, img_cols, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(510, 100, 100)\n",
      "(270, 100, 100)\n"
     ]
    }
   ],
   "source": [
    "# x_train = x_train.astype('float32')\n",
    "# x_test = x_test.astype('float32')\n",
    "\n",
    "# x_train /= 255\n",
    "# x_test /= 255\n",
    "\n",
    "\n",
    "y_train = keras.utils.np_utils.to_categorical(lbl_train, num_classes)\n",
    "y_test = keras.utils.np_utils.to_categorical(lbl_test, num_classes)\n",
    "\n",
    "\n",
    "#Convert x_train and x_test to tensors\n",
    "\n",
    "X_train = []\n",
    "X_test = []\n",
    "\n",
    "#for i in range(len(x_train)):\n",
    "#    X_train.append(tf.convert_to_tensor(x_train[i]))\n",
    "\n",
    "#for i in range(len(x_test)):\n",
    "#    X_test.append(tf.convert_to_tensor(x_test[i]))\n",
    "\n",
    "for arr in x_train:\n",
    "    X_train.append(tf.convert_to_tensor(arr))\n",
    "\n",
    "for arr in x_test:\n",
    "    X_test.append(tf.convert_to_tensor(arr))\n",
    "\n",
    "# X_train[18] = X_train[17]\n",
    "# X_train[28] = X_train[26]\n",
    "# X_test[6] = X_test[5]\n",
    "\n",
    "\n",
    "#tf.convert_to_tensor(X_train)\n",
    "#tf.convert_to_tensor(X_test)\n",
    "X_train = tf.stack(X_train, axis=0, name='stack1')\n",
    "X_test = tf.stack(X_test, axis=0, name='stack2')\n",
    "\n",
    "\n",
    "#print(X_train)\n",
    "#print(X_train[0])\n",
    "#print(X_train[18])\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 100)\n",
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train[18].shape)\n",
    "print(y_test[5].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_12 (Conv2D)          (None, 96, 96, 82)        2132      \n",
      "                                                                 \n",
      " average_pooling2d_12 (Avera  (None, 48, 48, 82)       0         \n",
      " gePooling2D)                                                    \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 47, 47, 128)       42112     \n",
      "                                                                 \n",
      " average_pooling2d_13 (Avera  (None, 23, 23, 128)      0         \n",
      " gePooling2D)                                                    \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 67712)             0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 500)               33856500  \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 300)               150300    \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 2)                 602       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34,051,646\n",
      "Trainable params: 34,051,646\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "10/10 [==============================] - 6s 589ms/step - loss: nan - accuracy: 0.1588 - val_loss: nan - val_accuracy: 0.1111\n",
      "Epoch 2/5\n",
      "10/10 [==============================] - 6s 574ms/step - loss: nan - accuracy: 0.1176 - val_loss: nan - val_accuracy: 0.1111\n",
      "Epoch 3/5\n",
      "10/10 [==============================] - 6s 573ms/step - loss: nan - accuracy: 0.1176 - val_loss: nan - val_accuracy: 0.1111\n",
      "Epoch 4/5\n",
      "10/10 [==============================] - 5s 555ms/step - loss: nan - accuracy: 0.1176 - val_loss: nan - val_accuracy: 0.1111\n",
      "Epoch 5/5\n",
      "10/10 [==============================] - 5s 556ms/step - loss: nan - accuracy: 0.1176 - val_loss: nan - val_accuracy: 0.1111\n",
      "{'loss': nan, 'accuracy': 0.1111111119389534}\n"
     ]
    }
   ],
   "source": [
    "## Define model ##\n",
    "model = Sequential()\n",
    "\n",
    "models_sizes = [0.000001, 0.00001, 0.00005, 0.0001, 0.001]\n",
    "\n",
    "# print(x_train[500])\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "model.add(layers.Conv2D(filters=82, kernel_size=(5, 5),\n",
    "          activation='relu', input_shape=(im_size - 2, im_size - 2, 1)))\n",
    "model.add(layers.AveragePooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(filters=128, kernel_size=(2, 2), activation='relu'))\n",
    "model.add(layers.AveragePooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dense(300, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=tensorflow.keras.optimizers.SGD(learning_rate=0.1),\n",
    "              metrics=['accuracy'],)\n",
    "\n",
    "fit_info = model.fit(X_train, y_train,\n",
    "                     batch_size=batch_size,\n",
    "                     epochs=epochs,\n",
    "                     verbose=1,\n",
    "                     validation_data=(X_test, y_test))\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0, return_dict=True)\n",
    "\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "almaEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
