{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN using fits files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import keras\n",
    "from keras import utils as np_utils\n",
    "import tensorflow\n",
    "from keras import datasets, layers, models\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preventing Out of memory error (OOM) - Idk why seems like good practice for Mac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU') # Grabbing all the GPUs avaiable on our machine\n",
    "\n",
    "# Limit the memory use for all of our GPUs\n",
    "for gpu in gpus: \n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find objects in fits images, rotating, etc.\n",
    "Requieres Clustar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "import joblib\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imread\n",
    "from init_modules import *\n",
    "\n",
    "\n",
    "# Set the path to the data directory\n",
    "data_set = {}\n",
    "im_size = 100\n",
    "shift_interval = 1\n",
    "\n",
    "# This function finds the position of the object in the image\n",
    "def find_object_pos(file):\n",
    "    cd = ClustarData(path=file, group_factor=0)\n",
    "    if len(cd.groups) > 0:\n",
    "        disk = cd.groups[0]\n",
    "        bounds = disk.image.bounds\n",
    "        x = (bounds[2] + bounds[3])/2\n",
    "        y = (bounds[0] + bounds[1])/2\n",
    "        return (x, y)\n",
    "    else:\n",
    "        # img_data = fits.getdata(file)\n",
    "        \n",
    "        # return img_data.data.shape[0]/2, img_data.data.shape[1]/2\n",
    "        print(\"No object found in {}\".format(file))\n",
    "        return None\n",
    "\n",
    "\n",
    "# This function crops the image to the size of the object\n",
    "def init_cropped_images(directory_of_fits_files):\n",
    "    fits_files = []\n",
    "    for fits_file in directory_of_fits_files:\n",
    "        img_data = fits.getdata(fits_file)\n",
    "        object_pos = find_object_pos(fits_file)\n",
    "\n",
    "        if object_pos != None:\n",
    "            # Data shape is (1, 1, x, y) we want it to be (x, y)\n",
    "            img_data.shape = (img_data.shape[2], img_data.shape[3])\n",
    "\n",
    "            # Set the size of the crop in pixels\n",
    "            crop_size = units.Quantity((im_size, im_size), units.pixel)\n",
    "\n",
    "            img_crop = Cutout2D(img_data, object_pos, crop_size)\n",
    "\n",
    "            fits_files.append(img_crop)\n",
    "\n",
    "    return fits_files\n",
    "\n",
    "\n",
    "# This function rotates the image by a random angle and shifts it by a random amount in a random direction\n",
    "def rotate_disk(disk_to_rotate, angle):\n",
    "\n",
    "    # Rotate the disk\n",
    "    rotated_disk = rotate(disk_to_rotate, angle)\n",
    "    # Since rotating pads the image, we need to crop it to the original size\n",
    "    x, y = (len(rotated_disk[0]), len(rotated_disk))\n",
    "\n",
    "    shift_interval = 0\n",
    "    si = shift_interval + 1\n",
    "\n",
    "    rand_x_shift = random.randint(-shift_interval, shift_interval)\n",
    "    rand_y_shift = random.randint(-shift_interval, shift_interval)\n",
    "\n",
    "    (x_lower, x_upper) = int((x/2 - im_size/2)) + \\\n",
    "        rand_x_shift, int(x/2 + im_size/2) + rand_x_shift\n",
    "    (y_lower, y_upper) = int((y/2 - im_size/2)) + \\\n",
    "        rand_y_shift, int(y/2 + im_size/2) + rand_y_shift\n",
    "\n",
    "    return rotated_disk[(x_lower+si):(x_upper-si), (y_lower+si):(y_upper-si)]\n",
    "\n",
    "\n",
    "# This function flips the image horizontally, vertically or both\n",
    "def flip_disk(disk_to_flip):\n",
    "\n",
    "    flipped_disk = disk_to_flip\n",
    "\n",
    "    if bool(random.getrandbits(1)):\n",
    "        flipped_disk = np.fliplr(flipped_disk)\n",
    "\n",
    "    if bool(random.getrandbits(1)):\n",
    "        flipped_disk = np.flipud(flipped_disk)\n",
    "\n",
    "    if bool(random.getrandbits(1)):\n",
    "        flipped_disk = np.flip(flipped_disk)\n",
    "\n",
    "    return flipped_disk\n",
    "\n",
    "\n",
    "# This function augments the image by rotating and flipping it\n",
    "def augment_disk(disk):\n",
    "    angle = random.randint(0, 360)\n",
    "    return rotate_disk(flip_disk(disk), angle)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate dataset from fits files\n",
    "Requieres Clustar since it runs the above functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# Generate dataset from the fits files\\n\\ndef generate_dataset(augmentations_per_gaussian, directory_of_fits_files):\\n    count = 0\\n    dataset = []\\n    fits_files = init_cropped_images(directory_of_fits_files)\\n    for fits_file in fits_files:\\n        for i in range(0, augmentations_per_gaussian):\\n            if len(fits_file.data) != 0:\\n                # zscale = ZScaleInterval(contrast=0.25, nsamples=1)\\n                \\n                # ret_data = zscale()\\n                dataset.append(augment_disk(fits_file.data))\\n            print(count)\\n            count += 1\\n    return dataset\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "# Generate dataset from the fits files\n",
    "\n",
    "def generate_dataset(augmentations_per_gaussian, directory_of_fits_files):\n",
    "    count = 0\n",
    "    dataset = []\n",
    "    fits_files = init_cropped_images(directory_of_fits_files)\n",
    "    for fits_file in fits_files:\n",
    "        for i in range(0, augmentations_per_gaussian):\n",
    "            if len(fits_file.data) != 0:\n",
    "                # zscale = ZScaleInterval(contrast=0.25, nsamples=1)\n",
    "                \n",
    "                # ret_data = zscale()\n",
    "                dataset.append(augment_disk(fits_file.data))\n",
    "            print(count)\n",
    "            count += 1\n",
    "    return dataset\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring/formatting of the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "# Hyper-parameters data-loading and formatting\n",
    "nmbr_of_aug = 15\n",
    "batch_size = 128\n",
    "num_classes = 2\n",
    "epochs = 3\n",
    "img_rows, img_cols = 82, 82  # sqrt of 6724\n",
    "\n",
    "x_train = np.array(generate_dataset(nmbr_of_aug, glob.glob('data/train_pos/*.fits')) +\n",
    "                   generate_dataset(nmbr_of_aug, glob.glob('data/train_neg/*.fits')))\n",
    "\n",
    "batch_size = int(len(x_train)/10)\n",
    "\n",
    "#print(len(x_train))\n",
    "#print(x_train[0])\n",
    "#print(x_train[0].shape)\n",
    "\n",
    "\n",
    "lbl_train = [0] * len(generate_dataset(nmbr_of_aug, glob.glob('data/train_pos/*.fits'))) + \\\n",
    "            [1] * len(generate_dataset(nmbr_of_aug, glob.glob('data/train_neg/*.fits')))\n",
    "\n",
    "\n",
    "x_test = np.array(generate_dataset(nmbr_of_aug, glob.glob('data/test_pos/*.fits')) +\n",
    "                  generate_dataset(nmbr_of_aug, glob.glob('data/test_neg/*.fits')))\n",
    "\n",
    "lbl_test =  [0] * len(generate_dataset(nmbr_of_aug, glob.glob('data/test_pos/*.fits'))) + \\\n",
    "            [1] * len(generate_dataset(nmbr_of_aug, glob.glob('data/test_neg/*.fits')))\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting our numpy arrays describing the fits files to a list of tensor and then making the entire list a tensor itself."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason will some of the augmented images be of a different size than the majority of the files, which we as of now can delete manually as we go. But this needs to be looked into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "y_train = keras.utils.np_utils.to_categorical(lbl_train, num_classes)\n",
    "y_test = keras.utils.np_utils.to_categorical(lbl_test, num_classes)\n",
    "\n",
    "\n",
    "#Convert x_train and x_test to tensors\n",
    "\n",
    "X_train = []\n",
    "X_test = []\n",
    "\n",
    "for arr in x_train:\n",
    "    X_train.append(tf.convert_to_tensor(arr))\n",
    "\n",
    "for arr in x_test:\n",
    "    X_test.append(tf.convert_to_tensor(arr))\n",
    "\n",
    "X_train = tf.stack(X_train, axis=0, name='stack1')\n",
    "X_test = tf.stack(X_test, axis=0, name='stack2')\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "## Define model ##\n",
    "model = Sequential()\n",
    "\n",
    "models_sizes = [0.000001, 0.00001, 0.00005, 0.0001, 0.001]\n",
    "\n",
    "# print(x_train[500])\n",
    "\n",
    "epochs = 82\n",
    "\n",
    "model.add(layers.Conv2D(filters=82, kernel_size=(5, 5),\n",
    "          activation='relu', input_shape=(98, 98, 1)))\n",
    "model.add(layers.AveragePooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(filters=128, kernel_size=(2, 2), activation='relu'))\n",
    "model.add(layers.AveragePooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dense(300, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=tensorflow.keras.optimizers.SGD(learning_rate=0.1),\n",
    "              metrics=['accuracy'],)\n",
    "\n",
    "fit_info = model.fit(X_train, y_train,\n",
    "                     batch_size=batch_size,\n",
    "                     epochs=epochs,\n",
    "                     verbose=1,\n",
    "                     validation_data=(X_test, y_test))\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0, return_dict=True)\n",
    "\n",
    "print(score)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
